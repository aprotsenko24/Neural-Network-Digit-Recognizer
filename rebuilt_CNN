import numpy as np
import matplotlib.pyplot as plt
from torchvision import datasets, transforms
MODELS=500
BACKPROP=1000
mnist = datasets.MNIST(root='./data', train=True, download=True, transform=transforms.ToTensor())
COUNTER=0
LAYER_SIZE=np.array([])
epsilon=1e-12
alpha=1e-1
learning_rate=0.04
decay_rate=1e-5
weights = np.empty(len(LAYER_SIZE)-1, dtype=object)
biases = np.empty(len(LAYER_SIZE)-1, dtype=object)
gradients=np.copy(weights)
delta=np.copy(biases)
train_models=np.empty(MODELS, dtype=object)
answers=np.zeros((MODELS, 10))

def add_layers(*layer_size):
    LAYER_SIZE=list()
    for layer in layer_size:
        LAYER_SIZE.append(layer)
        LAYER_SIZE=np.array(LAYER_SIZE)
def delete_layers(*indecies):
    for index in indecies:
        LAYER_SIZE=np.delete(LAYER_SIZE, index, axis=1)
def params_init():
    for i in range(len(LAYER_SIZE)-1):
        if i == len(LAYER_SIZE) - 2:
            weights[i] = np.random.randn(LAYER_SIZE[i+1], LAYER_SIZE[i]) * (1. / (LAYER_SIZE[i]))
            gradients[i] = np.zeros((LAYER_SIZE[i+1], LAYER_SIZE[i]))
            # He initialization
        else:
            weights[i] = np.random.randn(LAYER_SIZE[i+1], LAYER_SIZE[i]) * (2. / (LAYER_SIZE[i+1] + LAYER_SIZE[i]))
            gradients[i] = np.zeros((LAYER_SIZE[i+1], LAYER_SIZE[i]))
            # Xavier initialization
        biases[i]=np.full(LAYER_SIZE[i+1], 0)
        delta[i]=np.copy(biases)

def decay_func():
    learning_rate*=np.exp(-decay_rate)
def gradient_descend(gradient, delta):
    weights-=learning_rate*(gradient/train_models.__len__())
    biases-=learning_rate*(delta/train_models.__len__())
def Training_Models(Models):
    for i in range(Models):
        img_tensor, label = mnist[i]
        train_models[i]=img_tensor.view(-1).numpy()
        answers[i][label]=1
class CNN():
    def __init__(self, index):
        self.index=index
        self.a=np.empty(len(LAYER_SIZE), dtype=object)
        for i in range(len(LAYER_SIZE)):
            self.a[i]=np.zeros(LAYER_SIZE[i])
        self.z=np.copy(self.a[1:])
        self.a[0]=train_models[self.index]
    def feedforward(self):
        for i in range(len(LAYER_SIZE)-1):
            self.z[i]=np.dot(weights[i], self.a[i])+biases[i]
            if i==len(LAYER_SIZE)-2:
                self.a[i+1]=Softmax.func(self.z[i])
            else:
                self.a[i+1]=Leaky_ReLU.func(self.z[i], alpha)
    def backprop(self):
        gradeints=np.zeros_like(gradeints)
        delta=np.zeros_like(delta)
        for i in range(len(LAYER_SIZE)-1, 0, -1):
            if i==len(LAYER_SIZE)-1:
                delta[i-1]=(self.a[i]-answers[self.index])
            else:
                delta[i-1]=np.dot(weights[i].T, delta[i])*Leaky_ReLU.diff(self.z[i-1], alpha)
            gradeints[i-1]=np.outer(delta[i-1],self.a[i-1])+(alpha*weights[i-1])
class Softmax():
    """
    Softmax function used for feedforward and squishing the range of the number to 0-1
        Attributes:
            x (Any) - in my model, an array of latter layer neurons used for computing output layer neurons
            e_x (Any) - exponential value of x
            epsilon (float) - infinitesimal value to prevent NaN or inf values if there is division by 0
        Methods:
            func(x):
                Performs computations on the latter layer neurons and gives an output as the values for the last layer nuerons
    """
    @staticmethod
    def func(x):
        """"
        Performs computations on the latter layer neurons and gives an output as the values for the last layer nuerons
        Attributes:
            x (Any) - in my model, an array of latter layer neurons used for computing output layer neurons
            e_x (Any) - exponential value of x
            epsilon (int) - infinitesimal value to prevent NaN or inf values if there is division by 0
        Return values:
            Computes the output result for the last layer neurons
        """
        if np.any(np.isnan(x)) or np.any(np.isinf(x)):
            print("Wrong computations")
        e_x = np.exp(x - np.max(x))
        return e_x/(e_x.sum()+epsilon)

class ReLU():
    """
    Intemidiate function for the hidden layers, gives output for all layers' neuron except of the final output layer
    Attributes:
        z (Any) - input array of neurons
    Methods:
        func(z):
            Perfoms computations and gives output for ReLU function based on the input (usually, the previous layer neurons)
            Attributes:
                z (Any) - input array of neurons
            Return value:
                returns z if the sum for the specific layer is greater than zero, otherwise return 0
        diff(z):
            Performs differentiation w/r/t ReLU function
            Attributes:
                z (Any) - input array of neurons
            Return value:
                returns 1 if the sum for the specific layer is greater than zero, otherwise return 0
    """
    @staticmethod  
    def func(z):
        """
        func(x):
            Perfoms computations and gives output for ReLU function based on the input (usually, the previous layer neurons)
            Attributes:
                z (Any) - input array of neurons
            Return value:
                returns z if the sum for the specific layer is greater than zero, otherwise return 0
        """
        return np.where(z>0, z, z*0)
    @staticmethod
    def diff(z):
        return np.where(z>0, 1, 0)

class Leaky_ReLU():
    @staticmethod
    def func(z, alpha):
        """"
            Perfoms computations and gives output for ReLU function based on the input (usually, the previous layer neurons)
            Attributes:
                alpha (float) - a small float value for retaining the weights values small, not zeroed as it is in ReLU, if they are negative
                z (Any) - input array of neurons
            Return value:
                returns z if the sum for the specific layer is greater than zero, otherwise returns alpha times z

        """
        return np.where(z>0, z, alpha*z)
    @staticmethod
    def diff(z, alpha):
        """
            Performs differentiation w/r/t ReLU function
            Attributes:
                z (Any) - input array of neurons
                alpha (float) - a small float value for retaining the weights values small, not zeroed as it is in ReLU, if they are negative
            Return value:
                returns 1 if the sum for the specific layer is greater than zero, otherwise return alpha times z
        """
        return np.where(z>0, 1, alpha)
    
class Cost():
    """
    Cross-entropy loss function with L2 Regularization combined
    Attributes:
        a (Any) - input array of the predicted outputs
        y (Any) - input array with answers to the specific model
    Methods:
        func(a, y):
            Calculates the value for the cost function
            Attributes:
                a (Any) - input array of the predicted outputs
                y (Any) - input array with answers to the specific model
            Return values:
                the value for the cross-entropy loss function combined with L2 Regularization
    """
    @staticmethod
    def func(a, y):
        """
            Calculates the value for the cost function
            Attributes:
                a (Any) - input array of the predicted outputs
                y (Any) - input array with answers to the specific model
            Return values:
                the value for the cross-entropy loss function combined with L2 Regularization
        """
        entries_sum=sum(np.sum(w**2) for w in weights)
        return -np.sum(y*np.log(a[-1]+epsilon))+(alpha*entries_sum)

add_layers(784, 16, 10)
Train(MODELS)   # train_models[i] - image of the model in pixel brightness representation using array
                # answers[i] - an array representing an answer to the specific model with a single
                # index value identified by 1 for the answer number (answer number=index)
                # - You can change the number of train_models passed by passing a different parameter instead of const MODELS
cost=np.zeros(BACKPROP)
for k in range(BACKPROP):
    correct=0
    total_cost=0
    for i in range(train_models.__len__()):
        train_models[i]=CNN(i)
        train_models[i].feedforward()
        train_models[i].backprop()
        gradient_descend(gradients, delta)
        total_cost+=Cost.func(train_models[i].a, answers[i])
        if np.argmax(train_models[i].a[-1])==np.argmax(answers[i]):
            correct+=1
    cost[k]=total_cost/train_models.__len__()
    decay_func()
print((correct/train_models.__len__())*100)
plt.plot(cost)
# plt.show()
# By untagging the plt.show() function, you can see the trend of Loss function
